# **Fake News Detection by Supervised Fine Tuning of TinyLlama Model**
Fake news specifically refers to news reports that are untrue or exaggerated. These reports may be deliberately created to mislead the public or promote a specific agenda, through traditional media channels like print, and television as well as non-traditional media channels like social media. The wide and fast spread of fake news online has posed real-world threats in critical domains like politics, economy, and public health.<br /><br />
In this project, I built a fake news detection model by fine tuning TinyLlama  with Lora (Low rank adaption). TinyLlama is a compact 1.1B language model, built on the architecture and tokenizer of Llama 2. Instead of focusing solely on training computation-optimal language models, TinyLlama is an inference-optimal language models, aiming for optimal performance within specific inference constraints. The improved performance of TinyLlama is achieved by training models with more tokens than what is recommended by the scaling law. Following the same architecture and tokenizer as Llama 2, TinyLlama is obtained by training transformer decoder-only model with 1.1B parameters using approximately 3 trillion tokens.<br /><br />
The data are collected from Kaggle. I found 2 datasets with large number of ture and fake news classified by expert. After exhaustive data exploration, I found one dataset's ture news and fake news having potential character difference other than text difference. In such a case, I choose to work with a second one with 34k true news and 43k fake news. Baseline machine learning models such as logistic regression, decision tree, gradient boosting classifier and random forest are built and used as bench mark comparison. <br /><br />
After extensive experimentation, I found the most appropriate prompt format for TinyLlama train and validation. I built 3 training data set with 1000, 2000 and 5000 random sampling samples. I also built non-overlap vaildation dataset with 1000 random sampling samples. Both training data and validation dataset are uploaded to huggingface. In order to check the impact of length of text, I truncated the length of news article to 256 and 512 seprately. <br /><br />
TinyLlamma are downloaded from huggingface. I used supervised fine tuning to fine tune the TinyLlamma model based on the generated train dataset. I tried different number of epochs of 1 and 3.  <br /><br />
The performance with different parameters are compared with benchmark performance from traditional machine learing model. Interesing observations indicate the speical usefullness for larage language model in the problem of fake news detection. <br /><br />

## **File contents introduction:**<br /><br />

**1. Proposal for final project.pdf** is the proposal of the project, introducting the target and plan of this project. <br />
**2. fake-news-data-exploration3.ipynb** is data exploration, preprocessing and performance evalutation on traditional machine learning model. <br />
**3. Capstone3_fake_news_text_data_preprocessing.ipynb** is code for transforming text and label data into readable prompt format for TinyLlama model. <br />
**4. llama2_fakenews_detect_final.ipynb** containing code for TinyLlama model training and validation.<br />
**5. fake_news_detection_tinyllama.pdf** is powerpoint for presentation of the project.<br />
**6. Capstone_3_fake_news_detection.pdf** is the project report file.<br />
